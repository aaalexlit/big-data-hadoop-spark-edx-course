{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi97FfLfDZt3GHRkyBv+5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaalexlit/big-data-hadoop-spark-edx-course/blob/main/Data_Partitioning_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUnPbcxtTs7o",
        "outputId": "87a3fab3-c0f3-4fa7-f1b7-4a143edffe1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 37 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 56.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=7ea21d6be6e5cdfcb68260bce818678cf0bd4972ca890627b0a22acbfd8bb1f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "fRdwJvdVeiP-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "mxMt8S8nesbS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Context and Session"
      ],
      "metadata": {
        "id": "O50Ju2INe41a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "Z7qG8RYIe_kq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "feGTrScofIcu",
        "outputId": "ca665f5d-9a76-4b82-85de-70680370c7a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4075318940>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://14dd33f26f6a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDDs ang partitions\n",
        "\n",
        "`sc.parallelize()` creates RDD   \n",
        "`glom()` function gets the partitions"
      ],
      "metadata": {
        "id": "VhVU9uJ1fJVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = [i for i in range(10)]\n",
        "\n",
        "rdd = sc.parallelize(nums)\n",
        "\n",
        "print(f\"Number of partitions {rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {rdd.partitioner}\")\n",
        "print(f\"Partitions structure {rdd.glom().collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aYpjTMdfO_c",
        "outputId": "7ca03e68-c520-4f57-e422-ed52277ea86b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 2\n",
            "Partitioner None\n",
            "Partitions structure [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of elements in the RDD\n",
        "rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-7N846LgIrQ",
        "outputId": "04d06d37-ceb9-4ab6-ba9e-59736451104a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now parallelize into 15 partitions"
      ],
      "metadata": {
        "id": "6FrmkD2PgbXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(nums, 15)\n",
        "print(f\"Number of partitions {rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {rdd.partitioner}\")\n",
        "print(f\"Partitions structure {rdd.glom().collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQm249ucgiUX",
        "outputId": "e5b2f618-815c-4784-bc0e-e8e730a0b5ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 15\n",
            "Partitioner None\n",
            "Partitions structure [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partition using `partitionBy()`\n",
        "\n",
        " In this case, the dataset needs to be a tuple with a key/value pair as the default partioner uses a hash for the key to assign elements to a parition.\n",
        "\n"
      ],
      "metadata": {
        "id": "i1ebK5zLgodR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import I\n",
        "rdd = sc.parallelize(nums) \\\n",
        "        .map(lambda el: (el, el)) \\\n",
        "        .partitionBy(2) \\\n",
        "        .persist()\n",
        "\n",
        "print(f\"Number of partitions {rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {rdd.partitioner}\")\n",
        "print(f\"Partitions structure {rdd.glom().collect()}\")\n",
        "\n",
        "for i, partition in enumerate(rdd.glom().collect()):\n",
        "  print(f\"partition {i+1}: {partition}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd94yoP5gyaM",
        "outputId": "5b65ed8c-a82a-42ab-efb0-a0a221e44a89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 2\n",
            "Partitioner <pyspark.rdd.Partitioner object at 0x7f4075331a90>\n",
            "Partitions structure [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n",
            "partition 1: [(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)]\n",
            "partition 2: [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`parallelize(nums)` - transforming Python array into RDD with no partitioning scheme,    \n",
        "`map(lambda el: (el, el))` - transforming data into the form of a tuple,  \n",
        "`partitionBy(2)` - splitting data into 2 chunks using default hash partitioner"
      ],
      "metadata": {
        "id": "k18N5eE8hk-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More partitioning"
      ],
      "metadata": {
        "id": "V5uZniq7h6Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = [\n",
        "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
        "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
        "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
        "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
        "    {'name': 'Thomas', 'amount': 30, 'country': 'Germany'},\n",
        "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
        "    {'name': 'Pierre', 'amount': 120, 'country': 'France'},\n",
        "    {'name': 'Frank', 'amount': 180, 'country': 'France'}\n",
        "]"
      ],
      "metadata": {
        "id": "f2GtUw6BiWl9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If further analysis will be performed analyzing many similar records within the same country it makes sense to put records from one country in one node   \n",
        "For that a Custom partitioner is necessary  \n",
        "\n",
        "Custom partitioner — function returning an integer for given object (tuple key)"
      ],
      "metadata": {
        "id": "sByjlvCOiXHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_partitioner(country):\n",
        "  return hash(country) % (10**7 - 1)\n",
        "\n",
        "print(country_partitioner(\"Poland\"))\n",
        "print(country_partitioner(\"Germany\"))\n",
        "print(country_partitioner(\"United Kingdom\"))\n",
        "print(country_partitioner(\"France\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJfbitAE9jhd",
        "outputId": "84d40ad9-a117-4b93-c53a-b7c09522a732"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8909681\n",
            "5103677\n",
            "5521867\n",
            "2619581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "custom partitioner creates a unique hash for each country name so it can be passed as a parameter to `partitionBy`"
      ],
      "metadata": {
        "id": "CRcjKx6d933a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(transactions) \\\n",
        "      .map(lambda elem: (elem['country'], elem)) \\\n",
        "      .partitionBy(5, country_partitioner)\n",
        "\n",
        "print(f\"Number of partitions {rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {rdd.partitioner}\")\n",
        "print(f\"Partitions structure {rdd.glom().collect()}\")\n",
        "\n",
        "print(\"\\n--\\n\")\n",
        "\n",
        "for i, j in enumerate(rdd.glom().collect()):\n",
        "  print(f\"\\npartition {i + 1}:\\n{str(j)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuIYUime-IEX",
        "outputId": "5fd29935-71a2-4f1a-b95f-2012f6d16014"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 5\n",
            "Partitioner <pyspark.rdd.Partitioner object at 0x7f407429ea00>\n",
            "Partitions structure [[], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})], [('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})]]\n",
            "\n",
            "--\n",
            "\n",
            "\n",
            "partition 1:\n",
            "[]\n",
            "\n",
            "partition 2:\n",
            "[('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})]\n",
            "\n",
            "partition 3:\n",
            "[('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})]\n",
            "\n",
            "partition 4:\n",
            "[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})]\n",
            "\n",
            "partition 5:\n",
            "[('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "carry out calculations such as total revenue/sales "
      ],
      "metadata": {
        "id": "WcwQN8sP-yF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_sales(iterator):\n",
        "  yield sum(transaction[1]['amount'] for transaction in iterator)"
      ],
      "metadata": {
        "id": "lFUbe7hw_ElA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "by_country = sc.parallelize(transactions) \\\n",
        "             .map(lambda elem: (elem['country'], elem)) \\\n",
        "             .partitionBy(5, country_partitioner)\n",
        "          "
      ],
      "metadata": {
        "id": "nEU4Tcx-_VMC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_amounts = rdd.mapPartitions(sum_sales).collect()\n",
        "\n",
        "print(f\"Total sales for each partition: {sum_amounts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZao2uQE_sHW",
        "outputId": "52f03257-0bf6-414c-ba04-9acfbf048318"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sales for each partition: [0, 300, 126, 115, 230]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames\n",
        "Create a DataFrame from a python list"
      ],
      "metadata": {
        "id": "MKd7viCT_5dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(transactions)"
      ],
      "metadata": {
        "id": "6-DaMq8txP9D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfGS_vx0xURo",
        "outputId": "8e6a2341-faa5-405e-d432-afe291c275e6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+--------+\n",
            "|amount|       country|    name|\n",
            "+------+--------------+--------+\n",
            "|   100|United Kingdom|     Bob|\n",
            "|    15|United Kingdom|   James|\n",
            "|    51|        Poland|   Marek|\n",
            "|   200|       Germany|Johannes|\n",
            "|    30|       Germany|  Thomas|\n",
            "|    75|        Poland|    Paul|\n",
            "|   120|        France|  Pierre|\n",
            "|   180|        France|   Frank|\n",
            "+------+--------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of partitions {df.rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {df.rdd.partitioner}\")\n",
        "\n",
        "for i, partition in enumerate(df.rdd.glom().collect()):\n",
        "  print(f\"partition {i+1}: {partition}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwTaKP4xWU7",
        "outputId": "95c10821-ab17-4dc1-9695-b2e5297c03c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 2\n",
            "Partitioner None\n",
            "partition 1: [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')]\n",
            "partition 2: [Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-partition the DataFrame by country"
      ],
      "metadata": {
        "id": "SdtR2ybtxvFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.repartition(10, \"country\")\n",
        "\n",
        "print(f\"Number of partitions {df2.rdd.getNumPartitions()}\")\n",
        "print(f\"Partitioner {df2.rdd.partitioner}\")\n",
        "\n",
        "for i, partition in enumerate(df2.rdd.glom().collect()):\n",
        "  print(f\"partition {i+1}: {partition}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1t_umsnyKQD",
        "outputId": "87d73ff4-c535-4507-e2a4-507bea304362"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions 10\n",
            "Partitioner None\n",
            "partition 1: [Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n",
            "partition 2: []\n",
            "partition 3: [Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')]\n",
            "partition 4: []\n",
            "partition 5: [Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')]\n",
            "partition 6: [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')]\n",
            "partition 7: []\n",
            "partition 8: []\n",
            "partition 9: []\n",
            "partition 10: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VP9rWJKuypVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}