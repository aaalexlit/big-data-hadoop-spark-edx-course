{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKua3joFUAurhs4D4QdjhX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaalexlit/big-data-hadoop-spark-edx-course/blob/main/Getting_Started_With_Spark_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libs\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZL5hPreT7Ev",
        "outputId": "aa4c26d0-3242-4129-f383-1235d8515c0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.8/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to import pyspark as a normal library"
      ],
      "metadata": {
        "id": "1VfGU57lVMKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nh7RBSplTv7D"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark is the Spark API for Python. \n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "REUSB101T6G_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Context and Spark Session\n",
        "**SparkContext** is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`  \n",
        "**SparkSession** is needed for SparkSQL and DataFrame operations."
      ],
      "metadata": {
        "id": "hvtxYCNTTyKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the spark session and context"
      ],
      "metadata": {
        "id": "h33M09sOVjew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context \n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Python Spark DataFrames basic example\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "v4v2KtQeV4SS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Session\n",
        "verify that the spark session instance has been created"
      ],
      "metadata": {
        "id": "BotH_wnBWWSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Tq7l58L6X4Dd",
        "outputId": "2bd9b944-aeec-4b91-8964-a7c2bb1b825a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbf8b262040>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6958c0143b73:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resilient Distributed Datasets (RDDs)\n",
        "RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs.\n",
        "\n",
        "## Create and RDD\n",
        "Create and RDD from a python generator of integers from 1 to 30"
      ],
      "metadata": {
        "id": "bFo6nk1CX42i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = range(1, 30)\n",
        "print(data[0])\n",
        "len(data)\n",
        "xrangeRDD = sc.parallelize(data, 4)\n",
        "xrangeRDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nhj4oaeYxua",
        "outputId": "a30caeb9-09cd-4e02-a394-27e3747ccea4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations\n",
        "- A transformation is an operation on an RDD that results in a new RDD\n",
        "- Generated rapidly because the new RDD is lazily evaluated\n",
        "\n",
        "Transformations to the original RDD:\n",
        "1. reduce each element by 1\n",
        "2. filter to only contain elements < 10"
      ],
      "metadata": {
        "id": "_PPAQLaGZJ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subRDD = xrangeRDD.map(lambda x: x - 1)\n",
        "filteredRDD = subRDD.filter(lambda x: x < 10)"
      ],
      "metadata": {
        "id": "tyMJt-HNZujK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions\n",
        "To get the output from the transformation `collect()` action need to be applied"
      ],
      "metadata": {
        "id": "T0Oui4rnZ8bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(filteredRDD.collect())\n",
        "filteredRDD.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucwXopL5aOPj",
        "outputId": "65cf0be3-2b91-4d85-9f60-5490e6d8bf61"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching Data\n",
        "Create RDD and cache it.  \n",
        "gives **10x speed improvement**\n"
      ],
      "metadata": {
        "id": "-zF1ADy5aT88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "test = sc.parallelize(range(1, 50000), 4)\n",
        "test.cache()\n",
        "\n",
        "t1 = time.time()\n",
        "# first time it caches and counts\n",
        "count1 = test.count()\n",
        "dt1 = time.time() - t1\n",
        "print(\"dt1: \", dt1)\n",
        "\n",
        "t2 = time.time()\n",
        "# second time it only counts\n",
        "count2 = test.count()\n",
        "dt2 = time.time() - t2\n",
        "print(\"dt2: \", dt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbtz0_17aWwc",
        "outputId": "8b38328d-0955-4b08-d029-dc0965ce2ce8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt1:  1.4878273010253906\n",
            "dt2:  0.3539144992828369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames and SparkSQL\n",
        "To work with SQL engine we need to create a Spark Session.  \n",
        "It was already created, now to verify if it's still active"
      ],
      "metadata": {
        "id": "qlrcUrixbDmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "BGrOn0nZbxPF",
        "outputId": "d4999ecc-4cae-48de-83f8-ed4150136440"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbf8b262040>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6958c0143b73:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a DataFrame\n",
        "You can create a structured data set (much like a database table) in Spark.  \n",
        "Use powerful SQL tools to query and join dataframes"
      ],
      "metadata": {
        "id": "w-UGzOz0byBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data first into a local `people.json` file\n",
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQCZOTvVcBoW",
        "outputId": "4bfbb339-2076-456a-8ab5-de50d3a6eb9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    73  100    73    0     0    172      0 --:--:-- --:--:-- --:--:--   172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and cache the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(\"people.json\").cache()"
      ],
      "metadata": {
        "id": "lKOHPXTncGPc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrAT1-PecQiA",
        "outputId": "84ec8fe3-62d7-4068-cfc5-aac8de0d111b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createTempView(\"people\")"
      ],
      "metadata": {
        "id": "BsEKflApcXbU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data using DataFrame functions and SparkSQL\n",
        "Different ways to achieve the same task"
      ],
      "metadata": {
        "id": "1Vjct3excj0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "\n",
        "df.select(\"name\").show()\n",
        "df.select(df[\"name\"]).show()\n",
        "spark.sql(\"SELECT name FROM people\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX20-FVJctNe",
        "outputId": "6bdf933d-3f30-40e8-9257-20e77b9bbe34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic filtering\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJPrd2I5cuxx",
        "outputId": "71f4a37a-6cb7-4f5e-8a41-11a812caed8e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic aggregation\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, count(age) as num FROM people GROUP BY age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DEmV4ClnxZz",
        "outputId": "3538b835-9aa4-4c4f-f622-c4098bbe80cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    2|\n",
            "|null|    2|\n",
            "|  30|    2|\n",
            "+----+-----+\n",
            "\n",
            "+----+---+\n",
            "| age|num|\n",
            "+----+---+\n",
            "|  19|  2|\n",
            "|null|  0|\n",
            "|  30|  2|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "### Create an RDD with integers from 1-50. Apply a transformation to multiply every number by 2, resulting in an RDD that contains the first 50 even numbers.\n"
      ],
      "metadata": {
        "id": "jJqLDptloDLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = range(1, 50)\n",
        "numbers_RDD = sc.parallelize(numbers)\n",
        "even_numbers_RDD = numbers_RDD.map(lambda x: x * 2)"
      ],
      "metadata": {
        "id": "19Jqbg1Mocau"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similar to the `people.json` file, now read the `people2.json` file into the notebook, load it into a dataframe and apply SQL operations to determine the average age in our people2 file.\n"
      ],
      "metadata": {
        "id": "0R4xMlFCog68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
        "df = spark.read.json(\"people2.json\").cache()\n",
        "df.createTempView(\"people2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UmrPyD5om9b",
        "outputId": "6d381ce0-4b26-452e-c310-b47b09950ce6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT avg(age) from people2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TwtojQQo5ar",
        "outputId": "c4089a74-0854-46d2-af17-c6408c4cb7e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|         avg(age)|\n",
            "+-----------------+\n",
            "|24.77777777777778|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Close SparkSession"
      ],
      "metadata": {
        "id": "1TNp7NYUpBcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "q12gQBi8pXdP"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}